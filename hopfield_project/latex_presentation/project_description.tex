\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{times}

\title{Hopfield Networks: A Brain-Inspired Model for Associative Memory}
\author{
\textbf{Ingrid Corobana} \\
\texttt{ingrid-adriana.corobana@s.unibuc.ro} \\
\small Implementation, experiments
\and
\textbf{Cosmin Glod} \\
\texttt{cosmin.glod@s.unibuc.ro} \\
\small Mathematical analysis, presentation
\and
\textbf{Irina Moise} \\
\texttt{irina.moise@s.unibuc.ro} \\
\small Literature review, visualization
}
\date{Archaeology of Intelligent Machines --- January 2026}

\begin{document}

\maketitle

\begin{abstract}
This project explores Hopfield Networks through the lens of biological and physical analogies, presenting them as artificial "energy landscapes" where memories are stable valleys, mirroring protein folding dynamics and brain attractor states. We implement a Hopfield network from first principles, conduct experiments on capacity, noise robustness, and spurious attractors, and connect the classical model to modern extensions used in transformers.
\end{abstract}

\section{Introduction}

\subsection{Motivation}

The human brain performs remarkable feats of memory: we can recall a complete song from just a few notes, recognize a face from a partial glimpse, and fill in missing details of past experiences. These capabilities emerge from billions of neurons connected by trillions of synapses, yet the underlying computational principles can be captured by surprisingly simple models.

Hopfield Networks, introduced by John Hopfield in 1982 \cite{hopfield1982}, provide one such model. They demonstrate how a system of simple binary units, connected by symmetric weights and updated according to local rules, can exhibit emergent properties of associative memory. In 2024, Hopfield and collaborators were awarded the Nobel Prize in Physics for this work, recognizing its profound impact on both neuroscience and artificial intelligence.

\subsection{Project Philosophy: Brain-Inspired Analogies}

We will base the narrative of our project on biological and physical analogies, following recent popular explanations that use \textbf{protein folding} and \textbf{brain-inspired energy landscapes} as conceptual frameworks \cite{youtube_protein, youtube_hopfield}. 

\textbf{Key Analogy:} Just as proteins explore a high-dimensional configuration space and "roll downhill" to settle into low-energy folded states, Hopfield networks navigate a state space where stored memories correspond to energy minima (attractor states). Network dynamics drive the system toward these attractors, enabling memory retrieval from partial or noisy inputs.

In our implementation, each step (data representation, Hebbian learning, network dynamics, memory retrieval) will be presented both in mathematical/code form and using corresponding brain analogies (neurons, synapses, attractor states).

\subsection{Contributions Per Member}
\begin{itemize}
    \item \textbf{Ingrid Corobana:} Led implementation of the core \texttt{HopfieldNetwork} class in Python, designed and executed capacity/noise experiments, created the image retrieval notebook with Simpsons character faces, wrote project documentation and README files.
    \item \textbf{Cosmin Glod:} Derived mathematical foundations (energy function proofs, Hebbian rule derivation, pseudo-inverse learning rule), created the modern theory notebook connecting classical Hopfield to transformer attention, prepared the Beamer presentation slides.
    \item \textbf{Irina Moise:} Conducted literature review on Hopfield's original 1982 paper and modern extensions (Ramsauer et al. 2020), created visualizations (energy landscapes, pattern similarity matrices, 3D energy surfaces), documented biological analogies throughout.
\end{itemize}

\subsection{What Each Member Learned}
\textbf{Ingrid:} ``I learned how simple local update rules can produce emergent global behavior. Implementing the network from scratch gave me deep appreciation for how biological constraints (symmetric weights, no self-connections) lead to useful computational properties. In the future, I want to explore continuous Hopfield networks and their connection to modern attention mechanisms.''

\textbf{Cosmin:} ``Deriving the pseudo-inverse rule helped me understand why orthogonalization matters for memory capacity. The connection to transformers was surprising---attention is just modern Hopfield retrieval! I want to learn more about energy-based models and contrastive learning.''

\textbf{Irina:} ``Reading Hopfield's original paper showed me how physics concepts (energy, thermodynamics, spin glasses) apply to neural computation. The protein folding analogy made the mathematics intuitive. I want to explore Boltzmann machines and restricted Boltzmann machines next.''

\section{Background: What Has Been Done Before}

\subsection{Hopfield Networks (1982)}

Hopfield Networks are fully-connected recurrent neural networks where:
\begin{itemize}
    \item \textbf{Neurons}: Binary units $s_i \in \{-1, +1\}$ representing firing/silent states
    \item \textbf{Synapses}: Symmetric weights $w_{ij} = w_{ji}$ learned via Hebbian rule
    \item \textbf{Dynamics}: Asynchronous updates minimize an energy function
    \item \textbf{Memory}: Stored patterns become stable fixed points (attractors)
\end{itemize}

The energy function is defined as:
\begin{equation}
E(\mathbf{s}) = -\frac{1}{2} \sum_{i,j} w_{ij} s_i s_j - \sum_i \theta_i s_i
\end{equation}
where $\theta_i$ are neuron thresholds (often set to zero).

\textbf{Hebbian Learning Rule:}
\begin{equation}
w_{ij} = \frac{1}{N} \sum_{\mu=1}^{P} \xi_i^\mu \xi_j^\mu, \quad i \neq j, \quad w_{ii} = 0
\end{equation}
where $\boldsymbol{\xi}^\mu$ are the $P$ patterns to be stored, and $N$ is the number of neurons.

\textbf{Capacity:} For random uncorrelated patterns, the network can reliably store approximately $0.138N$ patterns \cite{hopfield1982, wiki_hopfield}.

\subsection{Modern Extensions}

Recent work has generalized Hopfield networks to achieve exponential capacity:
\begin{itemize}
    \item \textbf{Dense Associative Memory} (Krotov \& Hopfield, 2016): Higher-order interactions
    \item \textbf{Modern Hopfield Networks} (Ramsauer et al., 2020): Connection to attention mechanisms in transformers \cite{modern_hopfield}
\end{itemize}

These modern variants demonstrate that the core principles of energy-based associative memory remain relevant in contemporary deep learning architectures.

\section{Approach}

\subsection{Repository and Tools}
\begin{itemize}
    \item \textbf{GitHub Repository:} \url{https://github.com/dirgnic/Hopfield_Networks}
    \item \textbf{Google Colab Notebooks:}
    \begin{itemize}
        \item Project Description: \url{https://colab.research.google.com/drive/1ijqbNkXiPpae_7u4anCMlk6SQ_0sjzTM}
        \item Main Implementation: \url{https://colab.research.google.com/drive/18eRvSoLZ_0654VFzdFsTjFCBSbpGeku7}
    \end{itemize}
    \item \textbf{Software Tools:} Python 3.9+, NumPy, Matplotlib, scikit-learn (for PCA visualization), PIL (for image processing)
    \item \textbf{Hardware:} Standard laptop CPU (no GPU required)
    \item \textbf{Processing Time:} All experiments complete in under 1 minute on a standard laptop
\end{itemize}

\subsection{Biological Inspiration: From Proteins to Neurons}

\textbf{Act I — Energy Landscapes in Nature}

We begin by drawing an analogy to protein folding:
\begin{itemize}
    \item A protein chain explores a vast configuration space
    \item Energy landscapes guide the folding process
    \item Low-energy valleys correspond to stable folded states
    \item Nature solves optimization by "rolling downhill"
\end{itemize}

\textbf{Transition to Neural Systems:}
\begin{itemize}
    \item Replace protein configurations with neural firing patterns
    \item Energy valleys become stored memories (attractors)
    \item Brain dynamics = descent toward familiar states
\end{itemize}

\subsection{Implementation from First Principles}

\textbf{Act II — Building a Hopfield Network}

\subsubsection{Step 1: Neurons and Patterns}
\textbf{Implementation:}
\begin{itemize}
    \item Represent neuron states as $s_i \in \{-1, +1\}$
    \item Patterns are vectors $\boldsymbol{\xi} \in \{-1,+1\}^N$
    \item Example: 10×10 binary images (flattened to $N=100$ neurons)
\end{itemize}

\textbf{Brain Analogy:}
\begin{itemize}
    \item Each element = a neuron (firing or silent)
    \item A pattern = a global brain state snapshot
\end{itemize}

\subsubsection{Step 2: Hebbian Learning}
\textbf{Implementation:}
\begin{equation}
w_{ij} = \frac{1}{N} \sum_{\mu=1}^{P} \xi_i^\mu \xi_j^\mu, \quad i \neq j
\end{equation}

\textbf{Brain Analogy:}
\begin{itemize}
    \item Hebb's principle: "Cells that fire together, wire together"
    \item Synaptic plasticity strengthens co-active connections
\end{itemize}

\subsubsection{Step 3: Network Dynamics}
\textbf{Update Rule:}
\begin{equation}
s_i^{(t+1)} = \text{sign}\left(\sum_j w_{ij} s_j^{(t)} - \theta_i\right)
\end{equation}

\textbf{Energy Minimization:}\\
Asynchronous updates guarantee $\Delta E \leq 0$, so the system converges to a local minimum.

\textbf{Brain Analogy:}
\begin{itemize}
    \item Each neuron "listens" to synaptic inputs
    \item Positive input → fire; negative → stay silent
    \item System collectively settles into consistent configuration
\end{itemize}

\subsubsection{Step 4: Memory Retrieval}
\textbf{Implementation:}
\begin{enumerate}
    \item Add noise to a stored pattern (flip random bits)
    \item Initialize network with noisy input
    \item Run update rule until convergence
    \item Check if final state matches original pattern
\end{enumerate}

\textbf{Brain Analogy:}
\begin{itemize}
    \item "Hear a few notes, recall the whole song"
    \item Partial cue triggers complete memory
    \item Network "fills in" missing information
\end{itemize}

\subsection{Experiments and Evaluation}

\textbf{Act III — Testing the Model}

\subsubsection{Data and Exploratory Analysis}
\begin{itemize}
    \item \textbf{Data}: Synthetic 10×10 binary letter images (A, B, C, D, E)
    \item \textbf{EDA}: 
    \begin{itemize}
        \item Visualize stored patterns
        \item Compute Hamming distances between patterns
        \item Plot pattern similarity matrix (normalized overlap)
        \item Interpretation: High similarity → overlapping energy valleys → reduced capacity
    \end{itemize}
\end{itemize}

\subsubsection{Evaluation Metrics}
\begin{enumerate}
    \item \textbf{Retrieval Accuracy}: Fraction of noisy inputs correctly recovered
    \item \textbf{Noise Robustness}: Accuracy vs. noise level (10\%, 20\%, 30\%, etc.)
    \item \textbf{Capacity}: Number of patterns stored before performance collapse
    \item \textbf{Convergence}: Number of iterations to reach stable state
    \item \textbf{Energy Trajectory}: Visualization of energy minimization during retrieval
\end{enumerate}

\subsubsection{Capacity Experiment}
\textbf{Procedure:}
\begin{itemize}
    \item Store $P = 1, 2, \ldots, 30$ random patterns
    \item For each $P$, measure retrieval accuracy with 10\% noise
    \item Plot accuracy vs. $P$
    \item Compare to theoretical limit ($\sim 0.138N$)
\end{itemize}

\textbf{Brain Analogy:}\\
When too many memories are crammed into one energy landscape, valleys merge and the network creates "false memories" (spurious attractors).

\subsubsection{Spurious Attractors}
\textbf{Procedure:}
\begin{itemize}
    \item Store patterns near/above capacity
    \item Initialize with random states
    \item Check if network converges to non-stored patterns
    \item Visualize spurious attractors
\end{itemize}

\textbf{Brain Analogy:}\\
Like confabulation in human memory — the brain creates stable but incorrect memories by blending real experiences.

\subsection{Modern Connection: Transformers}

Briefly mention that modern Hopfield networks generalize the classical model and are mathematically equivalent to attention mechanisms in transformers \cite{modern_hopfield}. This connects our project to cutting-edge AI research.

\section{Limitations}

\begin{enumerate}
    \item \textbf{Limited Storage Capacity:} The classical Hopfield network can only store approximately $0.138N$ patterns reliably. For our 100-neuron network ($N=100$), this means only $\sim$14 memories---far too few for practical applications like image databases.
    
    \item \textbf{Binary Patterns Only:} Our implementation uses binary states $\{-1, +1\}$, which cannot directly represent continuous or grayscale data. Real images must be binarized, losing significant information. Modern continuous Hopfield networks address this but were not implemented.
    
    \item \textbf{Spurious Attractors:} The network creates ``false memories''---stable states that don't correspond to any stored pattern. These include mixture states (blends of stored patterns) and spin-glass states. We observed these experimentally but did not implement methods to suppress them.
    
    \item \textbf{No Temporal Sequences:} Classical Hopfield networks store static patterns only. They cannot learn or reproduce temporal sequences like melodies, motor actions, or video frames. Extensions like asymmetric Hopfield networks exist but were beyond our scope.
    
    \item \textbf{Scalability Issues:} The weight matrix grows as $O(N^2)$, making large networks memory-intensive. We tested up to $N=1000$ neurons but did not explore sparse connectivity or hierarchical architectures that might improve scaling.
    
    \item \textbf{Correlated Pattern Interference:} Highly similar patterns (e.g., letters ``O'' and ``Q'') interfere with each other, reducing effective capacity below the theoretical limit. While the pseudo-inverse rule helps, it requires $O(N^3)$ matrix inversion.
    
    \item \textbf{Synthetic Data Only:} Our experiments use synthetic letter patterns and preprocessed cartoon faces (Simpsons). We did not evaluate performance on natural images, handwritten digits (MNIST), or noisy real-world sensor data.
    
    \item \textbf{Incomplete Modern Theory Coverage:} While we discuss the connection to transformers, we did not implement the continuous modern Hopfield network from Ramsauer et al. (2020) that achieves exponential capacity.
\end{enumerate}

\section{Conclusions and Future Work}

\subsection{Reflections}

\textbf{What could we have done differently?}
\begin{itemize}
    \item We could have implemented the modern continuous Hopfield network with exponential capacity, which would have made the transformer connection more concrete
    \item More systematic comparison between Hebbian and pseudo-inverse learning rules across different pattern correlation levels
    \item GPU implementation using PyTorch for larger-scale experiments
    \item Interactive web demo using JavaScript/WebGL for real-time visualization
\end{itemize}

\textbf{How could this project be improved?}
\begin{itemize}
    \item Implement attention-based modern Hopfield retrieval from Ramsauer et al. (2020)
    \item Add stochastic updates (Boltzmann machine) to escape local minima
    \item Test on real image datasets (MNIST, CIFAR-10) with proper preprocessing
    \item Explore sparse connectivity patterns inspired by biological neural networks
    \item Add temporal sequence learning with asymmetric weights
\end{itemize}

\textbf{Did we enjoy this project?}

Yes! The elegant connection between physics, neuroscience, and computer science made this fascinating. Seeing the network ``recall'' a complete pattern from a corrupted input feels almost magical, yet it emerges from simple mathematical rules. The 2024 Nobel Prize recognition made us appreciate how foundational these ideas are to modern AI.

\textbf{What did we learn?}
\begin{itemize}
    \item Energy-based models provide powerful frameworks for understanding neural computation
    \item Simple local update rules can produce complex emergent behavior (content-addressable memory)
    \item Historical ideas from 1982 remain directly relevant to cutting-edge AI (transformers use Hopfield-like retrieval)
    \item The importance of combining mathematical analysis with empirical experiments
    \item How to structure a research project: literature review → implementation → experiments → analysis
\end{itemize}

\subsection{Suggestions for Future AMI Projects}
\begin{itemize}
    \item Provide a curated list of computational resources (Colab Pro, cloud GPU credits) for students wanting to scale up
    \item Include intermediate checkpoints/milestones to encourage incremental progress
    \item Brief LaTeX tutorial session for students unfamiliar with academic typesetting
    \item More class time for project discussions and peer feedback
    \item A shared repository of baseline implementations students can build upon
\end{itemize}

\section{References}

\begin{thebibliography}{9}
\bibitem{hopfield1982}
Hopfield, J. J. (1982). Neural networks and physical systems with emergent collective computational abilities. \textit{Proceedings of the National Academy of Sciences}, 79(8), 2554-2558.

\bibitem{modern_hopfield}
Ramsauer, H., Schäfl, B., Lehner, J., Seidl, P., Widrich, M., Adler, T., ... \& Hochreiter, S. (2020). Hopfield networks is all you need. \textit{arXiv preprint arXiv:2008.02217}.

\bibitem{youtube_protein}
"A Brain-Inspired Algorithm For Memory" (YouTube): \url{https://www.youtube.com/watch?v=1WPJdAW-sFo}

\bibitem{youtube_hopfield}
"Hopfield Networks Explained" (YouTube): \url{https://www.youtube.com/watch?v=piF6D6CQxUw}

\bibitem{wiki_hopfield}
Wikipedia: Hopfield Network. \url{https://en.wikipedia.org/wiki/Hopfield_network}
\end{thebibliography}

\end{document}
